What is the airflow?
What is a workflow?
Components in Airflow?
What are Local Executors and their types in Airflow?
What are Variables (Variable Class) in Apache Airflow?
What is the purpose of Airflow XComs?
Why don't we use Variables instead of Airflow XComs, and how are they different?
What are the states a Task can be in? Define an ideal task flow.
What is the role of Airflow Operators?-> operator can act as a wrapper around each unit of work that defines the actions that will be completed and minimizes or effort to write a lot of code
How does airflow communicate with a third party (S3, Postgres, MySQL)?
What are the basic steps to create a DAG?
What is Branching in Directed Acyclic Graphs (DAGs)?
What are ways to Control Airflow Workflow?BranchPythonOperator,LatestOnlyOperator,depends_on_past = true,trigger_rule[ "trigger_rule = always" to run it anyways.."trigger_rule = all_success" to run it only when all of its previous jobs succeed.]
Explain the External task Sensor?
What are the ways to monitor Apache Airflow?
What is TaskFlow API? and how is it helpful?
How are Connections used in Apache Airflow?
Explain Dynamic DAGs.
What are some of the most useful Airflow CLI commands?Airflow dags list,Airflow dags delete,Airflow DB init,Airflow DB check,Airflow tasks list
How to control the parallelism or concurrency of tasks in Apache Airflow configuration?parallelism: maximum number of tasks that can run concurrently |max_active_tasks_per_dag: maximum number of tasks that can be scheduled at once|max_active_runs_per_dag: . the maximum number of running tasks at once|concurrency: maximum number of tasks that can run concurrently in this dag.|max_active_runs: maximum number of active runs for this DAG.
What are Macros in Airflow?->Macros are functions used as variables
What are the limitations of TaskFlow API?-> small amt of data can be used 
List the types of Trigger rules->all_success,all_failed,all_done,one_failed,one_success,none_failed,none_skipped
What is Data Lineage?
What are SLAs?-> SLA stands for Service Level Agreement; this is a time by which a task or a DAG should have succeeded. 
Write a Python code to download a file from S3 to local system using airflow.
  from airflow import DAG
  from airflow.providers.amazon.aws.hooks.s3 import S3Hook
  from airflow.operators.python import PythonOperator
  from datetime import datetime
  S3_BUCKET = "your-bucket-name"
  S3_KEY = "path/to/your/file.csv"
  LOCAL_PATH = "/tmp/file.csv"
  AWS_CONN_ID = "aws_default"
  def download_from_s3():
      s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
      s3_hook.download_file(
          key=S3_KEY,
          bucket_name=S3_BUCKET,
          local_path=LOCAL_PATH
      )
      print(f"File downloaded to {LOCAL_PATH}")
Define a DAG and Schedule it to run on 10:30am everyday, starting from 1st November 2022.
  with DAG(
      dag_id="daily_1030am_dag",
      start_date=datetime(2022, 11, 1),
      schedule_interval="30 10 * * *",  # 10:30 AM every day
      catchup=False,  # Set True if you want backfills from Nov 1, 2022
      tags=["daily", "schedule_example"],
  ) as dag:
What is a Spark Submit Operator?
  spark_task = SparkSubmitOperator(
        task_id="run_spark_job",
        application="/path/to/your_spark_script.py",
        conn_id="spark_default",
        executor_memory="2g",
        driver_memory="1g",
        num_executors=2,
        verbose=True
    )
What Executor will you use to test multiple jobs at a low scale?-> Local Executor is ideal for testing multiple jobs in parallel for performing tasks
If we want to exchange large amounts of data, what is the solution to the limitation of XComs?->spark
What will happen if you set 'catchup=False' in the dag and 'latest_only = True' for some of the dag tasks?->Since in the dag definition, we have set catchup to False, the dag will only run for the current date
Is there any way to restrict the number of variables to be used in your directed acyclic graph, and why would we need to do that?->we can just use a single Airflow variable as a JSON, as an Airflow variable can contain JSON values 
How would you approach if you wanted to queue up multiple dags with order dependencies?->We can use External task sensors 
How would you handle a task which has no dependencies on any other tasks?->trigger_rules = 'always'
How can you use a set or a subset of parameters in some of the dags tasks without explicitly defining them in each task?->{{params.param1}}
What will happen if you set 'catchup=False' in the dag and 'latest_only = True' for some of the dag tasks?->Since in the dag definition, we have set catchup to False, the dag will only run for the current date
What if your Apache Airflow DAG failed for the last ten days, and now you want to backfill those last ten days' data, but you don't need to run all the tasks of the dag to backfill the data?->We can use the Latest Only (LatestOnlyOperator) 
